# How good is my assembly?

:::::::::::::: {.columns align="top"}
::: {.column width="0.5"}

**Assembly qualities** (jargon!):

- draft
- reference
- chromosome-level
- complete/closed, telomere to telomere (T2T)

:::
::: {.column width="0.5"}

**High-quality genomes** have^1^:

- contig N~50~ length ≥ 1 Mb
- scaffold N~50~ length ≥ 10 Mb
- ≥ 90% of sequence assigned to chromosomes
- error rate ≤ 0.01% (1 error in 10,000 bases)

\vspace{2ex}

\scriptsize

^1^ according to the VGP

:::
::::::::::::::

::: notes
\tiny
- you've done an assembly using one of the approaches i talked about in the last video
- ideally you've used a bunch of different approaches
- how are you going to compare them to decide which one is the most useful?
- there are a bunch of words that are used to describe genome assemblies
- most of this jargon used pretty loosely so these aren't formal definitions at all
- a draft genome probably means you've got a fragmented assembly in large number of contigs, and probably haven't done any scaffolding
- you might not expect to have a very complete sequence in that situation
- reference assembly  would imply that you have some larger scaffolds, probably corresponding to chromosomes, and you've got some evidence that the sequence is 90% complete or more
- chromosome-level means that you've scaffolded most of your contigs into chromsomes, and you have a sequence for each chromosome
- these assemblies almost always still have some unanchored contigs that couldn't be assigned to chromosomes
- a telomere to telomere or complete assembly contains the whole sequence of the chromosome, with the word closed meaning that the gaps have been filled in
- there have been some attempts to standardise assembly quality metrics but the goalposts do shift every now and then as sequencing technology improves and gets cheaper
- the vertebrate genome project (VGP) is aiming to assemble 70 thousand vertebrate genomes
- they have set some quality targets for these assemblies
- they're aiming at contig N50 length at least 1 Mb and scaffold N50 length at least 10 Mb
- I'll explain what N50 means in a moment
- they want at least 90% of sequence assigned to chromosomes and less than 1 error every ten thousand bases pairs in the assembly

:::

# *N*~50~ is a contiguity statistic

![](img/Figure1b.jpg)

\vspace{1ex}

\centering

***The sequence length of the shortest contig at 50% of the total genome length***

\vspace{1ex}

\centering

Is this *N*~50~ (*N* \rightarrow \ number)?  
Or *L*~50~ (*L* \rightarrow \ length)?  
***N~50~ length***?  
\large \emojione

\source{
     \href{https://www.molecularecologist.com/2017/03/29/whats-n50/}{The Molecular Ecologist}
}

::: notes
\tiny

- N50 is a bit of a pain to get your head around
- it's just a statistic that is supposed to give you an idea of the distribution of fragment sizes in your assembly
- it's not that different to the median or mean length, but it just gives a bit more weight to the longer contigs
- the technical definition is the sequence length of the shortest contig at 50\% of the total genome length
- weighted median statistic such that 50\% of the entire assembly is contained in contigs or scaffolds equal to or larger than the N50 value
- it's pretty counter intuitive to see written like that so i'll show you an example
- let's imagine this is our genome assembly made up of these 7 contigs which are labelled with the length of the contigs
- the contigs are shown here ordered from longest to shortest
- if we sum the contig lengths we get 400 bases
- so the N50 contig is going to be the shortest contig that includes 200 b of the assembly
- these three contigs include 230 b, so the N50 here is the length of the shortest contig
- in this case that's going to be 60
- now as if that's not complicated enough, unfortunately there's a bit of mixed usage with N50
- some people would say the N50 here is actually three, because the three longest contigs make up 50% of the assembly
- in that case the L50 is the length of the N50 contig, so L50 would be 60
- i'm afraid there's no consistent usage and i've also seen these terms used the other way around with L50 referring to the number of contigs
- I try to use the phrase N50 length so it's obvious that I'm talking about a contig length rather than a number of contigs
- this might not be 100\% clear to you yet, and you have my commiseration. it's a tricky concept to grasp
- remember that it's a statistic for describing the distribution of fragment sizes, just like the mean contig length is
- a high N50 means that your genome is assembled into large chunks, and a low N50 means the assembly is quite scaffolded
- it doesn't measure the underlying quality of the assembly
- for example if you wanted a great N50, you could just paste all the chunks together. the sequences would be wrong, but the N50 would be high
:::

# Naïve *N*~50~ in `python3`

\footnotesize

:::::::::::::: {.columns }
::: {.column width="0.5"}

```{python part1, eval=TRUE, echo=TRUE, prompt = TRUE, comment = "", collapse = FALSE, autodep=TRUE, cache=FALSE}
#!/usr/bin/env python3

import numpy as np

contig_lengths = [
  40, 100, 70, 50, 60, 50, 30]
sorted_lengths = sorted(
  contig_lengths,
  reverse=True)

genome_size = sum(sorted_lengths)
print(genome_size)
```

:::
::: {.column width="0.5"}

```{python part2, eval=TRUE, echo=TRUE, prompt = TRUE, comment = "", collapse = FALSE, dependson='part1', autodep=TRUE, cache=FALSE}
i = [
  x >= genome_size * 0.5
  for x in np.cumsum(sorted_lengths)
  ].index(True)

print(i)
sorted_lengths[i]
```

:::
::::::::::::::

::: notes
\tiny
- this is exactly what i said on the previous slide except in python code rather than a cartoon
- it's a naive implementation using numpy to calculate the n50 length from a bunch of contig lengths
- these are the same as the values on the previous slide
- all we have to do is sort them from largest to smallest (that's what reverse is for)
- the genome size is the sum of the lengths
- this is a one-liner to get the n50 contig (not the lengths)
- we use numpy's cumulative sum function to add up the contig lengths
- the first element will be 100, the second will be 100 + 70 and so on
- and then for all the elements in that result, we just check if it's greater than or equal to half the genome
- and then we use the index method to get the first item where this evaluates to True
- that's at this location in the list
- we look up the length of that contig is 60, just like on the previous slide
:::


# How do we know if we have a good assembly?

```{r metrics}
metrics <- fread("data/metrics.csv",
           skip = 1,
           select = 2:6,
           header = TRUE)

pander::pander(metrics)
```
\vspace{1ex}

\scriptsize\centering

*N*~50~ describes the distribution of contig or scaffold sizes.  
Higher *N*~50~ means the assembly is in bigger chunks.  

\source{
VPG assembly metrics, \href{https://doi.org/10.1101/2020.05.22.110833}{Rhie \emph{et al}., 2020}
}

::: notes
\tiny
- back to the VGP's metrics for the various levels of genome assembly
- apart from the N50 stats that i just told you about, they also use
- the number of gaps in the assembly
- the completeness, based on the fraction of k-mers from high quality reads that are found in the finished genome
- gene-based completeness, which is measured by searching for certain highly-conserved genes from related species
- that's commonly done with software called BUSCO
- and mappability of transcripts from related species
- as you'd expect in a draft assembly the n50 stats are quite low and the genome is probably only 80\% complete or so, based on these other metrics
- along with the target n50 stats i metioned, the VGP also aims for completeness over 90\%
- the third column shows the range of quality scores of actual assemblies in their database
- their assemblies are in a pretty good state although some of them need additional work to close gaps, which will proably also help the completeness stats
- and just for comparison, the last column shows that a truly complete assembly would look like
- although none of these exist for complex eukaryotic genomes
- the contigs and scaffolds would be the same, and the n50 length would just be the n50 length of the chromosomes themselves
- a completed assembly would not have any gaps, and all of the kmers from the high quality reads would be found in the assembly
- there is an upper limit on the gene completeness and transcript mappability, because of genuine differences between the organisms used for comparison
:::

# Scaffolding

:::::::::::::: {.columns align="center"}
::: {.column width="0.4"}

![](img/13059_2019_1829_Fig1_HTML_crop.png){width=0.4\textwidth}

:::
::: {.column width="0.6"}

- **reference genome**
- mate pairs
- long reads with a short-read assembly
- proximity ligation
- optical mapping

:::
::::::::::::::

\sourceleft{
     \href{https://doi.org/10.1186/s13059-019-1829-6}{Alonge \emph{et al}., 2019}
}

::: notes
\tiny
- if you have an assembly that you're trying to improve, one way it so go back and get more, longer reads and repeat the assembly
- that's not always feasible and it's not guaranteed to improve the assembly
- another common approach is the use complementary data sources to join contigs together
- these approaches are pretty effective at joining gaps so they're often done in parallel to the 
- what are some methods of scaffolding these contigs?
- one option is to simply align your contigs to a reference genome and use that to decide the order of contigs and the size of the gaps between them
- this is probably the least satisfactory method because it ignores the potential for genuine differences from the references in the arrangement of the genome you are trying to assemble
- sometimes you don't have any other data and this is still useful to arrange your contigs in an orientation that matches the reference, even if you don't join the contigs into scaffolds
:::

# Scaffolding

:::::::::::::: {.columns align="center"}
::: {.column width="0.4"}

![](img/PET_contig_scaffold.png){width=0.4\textwidth}

:::
::: {.column width="0.6"}

- reference genome
- **mate pairs**
- **long reads with a short-read assembly**
- proximity ligation
- optical mapping
- linkage mapping


:::
::::::::::::::

\sourceleft{
     \href{https://commons.wikimedia.org/wiki/File:PET_contig_scaffold.png}{University of California via Wikimedia Commons}
}

::: notes
\tiny

- another technique is to use what are called mate pairs to try to anchor distant scaffolds
- in this technique you generate large fragments of DNA of a known size, and just use Illumina sequencing to sequence the ends of the fragments
- because you know the average fragment size, you can guess how far apart the bits you sequence were in the genome, so you can use this to link contigs over gaps that are roughly smaller than the fragment size
- using this method allows you to link the contigs, but it doesn't fill in the sequence of the gap, so you are still left with a string of Ns in the gap
- actually this method is not as popular as it was a few years ago
- because long read sequencing is now pretty cheap and easy to do, it's straightforward just to sequence the entire fragment, which means you can link the contigs together and fill in the gap
:::

# Scaffolding

:::::::::::::: {.columns align="center"}
::: {.column width="0.4"}

![](img/hic.jpg){width=0.4\textwidth}

:::
::: {.column width="0.6"}

- reference genome
- mate pairs
- long reads with a short-read assembly
- **proximity ligation**
- optical mapping
- linkage mapping

:::
::::::::::::::

\sourceleft{
     \href{https://dx.doi.org/10.1126/science.1181369}{Lieberman-Aiden \emph{et al}., 2009}
}

::: notes
\tiny
- another scaffolding technique is proximity ligation
- you sometimes hear these techniques called Hi-C or chromatin conformation capture sequencing
- these are available as commercial pipelines and it's proven to be a very successful scaffolding technique, so you might also hear it called Proximo sequencing or Dovetail sequencing, these are the commercial names
- it relies on a couple of molecular biology tricks so it's a bit counter-intuitive, 
- the basic principle is that the DNA is immobilised in place in the cell by something called chromatin cross-linking
- cross-linking joins pairs of dna molecules that are physically close together 
- the bits of DNA are trimmed near the cross-linking site
- the pairs of DNA molecules are linked together by ligation
- and then the junction between the two molecules is sequenced usually by illumina sequencing
- so when you get your sequencing read and you find a bit of the blue sequence and a bit of the red sequence, that tells you that those two sequences were physically close together
- so it's a way of measuring how close together two bits of DNA are in the cell
- on average, over a large number of cells, the fragments that are closer together in genome have a higher probability of being linked together, so this method is actually a way of measuring physical proximity of short sequence tags in the genome
- and so pairs of contigs that have a high number of interactions will be likely to be closer together
- and that allows you to do scaffolding by joining the pair of contigs that have the highest number of interactions, then the pair that has the second highest number of interactions, and so on, until you have the number of clusters of contigs that corresponds to the number of chromosomes in the genome
- so for example if you were scaffolding a drosophila genome like this, you'd keep joining contigs until you had 4 clusters, because that's the number of chromosomes we expect to find in the Drosophila genome
- this method has been really useful for scaffolding fragmented genomes
- because it allows you to generate chromosome-scale scaffolds
- however like mate-pair sequencing, it doesn't fill in any sequence in the gaps
:::

# Scaffolding


:::::::::::::: {.columns align="center"}
::: {.column width="0.4"}

![](img/bionano.png){width=0.4\textwidth}

:::
::: {.column width="0.6"}

- reference genome
- mate pairs
- long reads with a short-read assembly
- proximity ligation
- **optical mapping**
- **linkage mapping**

:::
::::::::::::::

\sourceleft{
     \href{https://doi.org/10.7717/peerj.10150}{Istace \emph{et al}., 2020}
}

::: notes
\tiny
- another scaffolding method that has been quite popular recently is optical mapping
- this is also avialable as a commercial pipeline, so you might hear it called bionano
- the way this works is that the dna molecules are tagged using sequence-specific fluorescent tags
- then the individual DNA molecules are imaged to create maps of the tags across the molecules
- so in this diagram the bars represent sites that have been tagged
- we can see from the order of sites matches on these three DNA molecules so they can be joined together into a single scaffold
- again we don't get the sequence information in the gap that's shown in red here, we only know that the two contigs should be joined together
- the final scaffolding method that i'll talk about is linkage mapping
- it's the exact same principle as optical mapping, except using genetic markers instead of sequence tags in the DNA
- so if you have a set of markers that have been mapped with classical genetic mapping, and you can find the sequence of the markers in your contigs, you can use the genetic information to join the contigs into scaffolds
- it's probably a bit unusual to have this information these days, but if you're working with e.g. an agriculturally relevent plant genome or a livestock species, it's possible that a genetic map has been constructed for it at some point, and if you have it it can be a pretty accurate scaffolding technique
- one thing to keep in mind is that out of all of the scaffolding techniques, long reads are the only approach that addresses the sequence of the gaps
- so for a truly complete sequence of any genome that has regions that can't be assembled using illumina sequencing, at some point you are going to need to use long reads
- and this approach is being used now to try to get a fully closed sequence of the human genome
- this approach where even the repetitive elements of the genome are fully sequenced is being called telomere-to-telomere assmebly
:::

# Summary

\centering

**Two major approaches to *de novo* assembly**:

\vspace{1ex}

:::::::::::::: {.columns align="top"}
::: {.column width="0.5"}

**Short read**

- prokaryotes, simple genomes
- relatively cheap
- high accuracy assemblies, but struggles with complex genomes
- assembly algorithms are fast and efficient

:::
::: {.column width="0.5"}

**Long read**

- resolves complicated genomes
- expensive, relatively tricky to generate data
- accuracy a little lower
- assembly is slow and memory-hungry

:::
::::::::::::::

\vspace{2ex}

Scaffolding improves the contiguity in many cases

Hybrid approaches are currently popular

::: notes
\tiny
- to summarise this lecture
- after i introduced the concepts of genome assembly, i went through two major classes of genome assembly
- short read genomes using second-generation illumina sequencing are good for prokaryotes or simple genomes without too much repeat content
- the main benefit is that it is relatively cheap to produce a lot of high quality data, but the limitations of the short reads mean that assembly of complex repetitive regions isn't possible
- the high quality and short sequences mean that software for these assemblies can use efficient de bruijn graph algorithms
- in contrast long read assemblies are better for resolving complex genomes
- but the data is a little harder to come by because sequencing is expensive and you have to be very careful with the sample
- accuracy is a little lower but it's catching up
- probably only a matter of time until these are as accurate as short read assemblies - there are alread ONT assemblies of bacterial genomes that are don't have any base errors
- the software for assembling these datasets is much slower and uses much more memory than short read assembly
- i also talked about a bunch of scaffolding techniques that can be combined with any assembly approach
- recently hybrid assemblies have been popular for combining short read data with long read data
- personally i wonder if the increasing accuracy of long reads means that this approach will become redundant
- but i'll talk about an example of a hybrid assembly in the live session on thursday
:::

